{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv16HppWUeva"
   },
   "source": [
    "## Watch Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1sd9SzuUvob",
    "outputId": "ca538086-e60a-4002-bab5-ba8de91f7991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
      "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
      "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
      "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
      "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
      "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Fetched 88.7 kB in 3s (34.1 kB/s)\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.2  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "spark_version = 'spark-3.2.2'\n",
    "\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHfFQiF_i6S5",
    "outputId": "3dd7bfec-a728-4330-8259-c972ebec0b0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-21 00:09:22--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
      "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
      "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 914037 (893K) [application/java-archive]\n",
      "Saving to: ‘postgresql-42.2.9.jar.3’\n",
      "\n",
      "postgresql-42.2.9.j 100%[===================>] 892.61K  1.26MB/s    in 0.7s    \n",
      "\n",
      "2022-11-21 00:09:23 (1.26 MB/s) - ‘postgresql-42.2.9.jar.3’ saved [914037/914037]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install postgressql \n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "nPRmMt8FVirP"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"WatchReviews\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVW7jBoKUeve",
    "outputId": "eee1652d-dec4-49ad-ceab-f4fba0f0cd3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "|         US|    3653882|R3O9SGZBVQBV76|B00FALQ1ZC|     937001370|Invicta Women's 1...|         Watches|          5|            0|          0|   N|                Y|          Five Stars|Absolutely love t...| 2015-08-31|\n",
      "|         US|   14661224| RKH8BNC3L5DLF|B00D3RGO20|     484010722|Kenneth Cole New ...|         Watches|          5|            0|          0|   N|                Y|I love thiswatch ...|I love this watch...| 2015-08-31|\n",
      "|         US|   27324930|R2HLE8WKZSU3NL|B00DKYC7TK|     361166390|Ritche 22mm Black...|         Watches|          2|            1|          1|   N|                Y|           Two Stars|           Scratches| 2015-08-31|\n",
      "|         US|    7211452|R31U3UH5AZ42LL|B000EQS1JW|     958035625|Citizen Men's BM8...|         Watches|          5|            0|          0|   N|                Y|          Five Stars|It works well on ...| 2015-08-31|\n",
      "|         US|   12733322|R2SV659OUJ945Y|B00A6GFD7S|     765328221|Orient ER27009B M...|         Watches|          4|            0|          0|   N|                Y|Beautiful face, b...|Beautiful watch f...| 2015-08-31|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Watches_v1_00.tsv.gz\"), sep=\"\\t\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BXQWriTIrRI"
   },
   "source": [
    "Change Datatype for functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "qZ64gSMy7dfw"
   },
   "outputs": [],
   "source": [
    "# Import struct fields that we can use\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFfVvZDl7dlg",
    "outputId": "a5f7848b-76c7-48f0-ce21-85724b2ab15e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(marketplace,StringType,true),\n",
       " StructField(customer_id,IntegerType,true),\n",
       " StructField(review_id,StringType,true),\n",
       " StructField(product_id,StringType,true),\n",
       " StructField(product_parent,IntegerType,true),\n",
       " StructField(product_title,StringType,true),\n",
       " StructField(product_category,StringType,true),\n",
       " StructField(star_rating,IntegerType,true),\n",
       " StructField(helpful_votes,IntegerType,true),\n",
       " StructField(total_votes,IntegerType,true),\n",
       " StructField(vine,StringType,true),\n",
       " StructField(verified_purchase,StringType,true),\n",
       " StructField(review_headline,StringType,true),\n",
       " StructField(review_body,StringType,true),\n",
       " StructField(review_date,StringType,true)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we need to create the list of struct fields\n",
    "schema = [StructField(\"marketplace\", StringType(), True), StructField(\"customer_id\", IntegerType(), True), StructField(\"review_id\", StringType(), True), StructField(\"product_id\", StringType(), True), StructField(\"product_parent\", IntegerType(), True), StructField(\"product_title\", StringType(), True), StructField(\"product_category\", StringType(), True), StructField(\"star_rating\", IntegerType(), True), StructField(\"helpful_votes\", IntegerType(), True), StructField(\"total_votes\", IntegerType(), True), StructField(\"vine\", StringType(), True), StructField(\"verified_purchase\", StringType(), True), StructField(\"review_headline\", StringType(), True), StructField(\"review_body\", StringType(), True), StructField(\"review_date\", StringType(), True)]\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qnxTaDZY7dot",
    "outputId": "c8e8a730-95f4-4057-e653-37b552246546"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(marketplace,StringType,true),StructField(customer_id,IntegerType,true),StructField(review_id,StringType,true),StructField(product_id,StringType,true),StructField(product_parent,IntegerType,true),StructField(product_title,StringType,true),StructField(product_category,StringType,true),StructField(star_rating,IntegerType,true),StructField(helpful_votes,IntegerType,true),StructField(total_votes,IntegerType,true),StructField(vine,StringType,true),StructField(verified_purchase,StringType,true),StructField(review_headline,StringType,true),StructField(review_body,StringType,true),StructField(review_date,StringType,true)))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass in our fields\n",
    "final = StructType(fields=schema)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lS4Jf2tp7drZ",
    "outputId": "eb41ed06-9c1a-4adc-c38d-b0f4514a8213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "|         US|    3653882|R3O9SGZBVQBV76|B00FALQ1ZC|     937001370|Invicta Women's 1...|         Watches|          5|            0|          0|   N|                Y|          Five Stars|Absolutely love t...| 2015-08-31|\n",
      "|         US|   14661224| RKH8BNC3L5DLF|B00D3RGO20|     484010722|Kenneth Cole New ...|         Watches|          5|            0|          0|   N|                Y|I love thiswatch ...|I love this watch...| 2015-08-31|\n",
      "|         US|   27324930|R2HLE8WKZSU3NL|B00DKYC7TK|     361166390|Ritche 22mm Black...|         Watches|          2|            1|          1|   N|                Y|           Two Stars|           Scratches| 2015-08-31|\n",
      "|         US|    7211452|R31U3UH5AZ42LL|B000EQS1JW|     958035625|Citizen Men's BM8...|         Watches|          5|            0|          0|   N|                Y|          Five Stars|It works well on ...| 2015-08-31|\n",
      "|         US|   12733322|R2SV659OUJ945Y|B00A6GFD7S|     765328221|Orient ER27009B M...|         Watches|          4|            0|          0|   N|                Y|Beautiful face, b...|Beautiful watch f...| 2015-08-31|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read our data with our new schema\n",
    "table = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Watches_v1_00.tsv.gz\"), schema=final, sep=\"\\t\", header=True)\n",
    "table = table.withColumn(\"review_date\", table[\"review_date\"].cast(\"date\"))\n",
    "table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKMW3LWg6dLi",
    "outputId": "adf99985-c339-4a7e-adcf-203c4538064b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+--------------------+-----------+\n",
      "|star_rating|helpful_votes|total_votes|         review_body|review_date|\n",
      "+-----------+-------------+-----------+--------------------+-----------+\n",
      "|          5|            0|          0|Absolutely love t...| 2015-08-31|\n",
      "|          5|            0|          0|I love this watch...| 2015-08-31|\n",
      "|          2|            1|          1|           Scratches| 2015-08-31|\n",
      "|          5|            0|          0|It works well on ...| 2015-08-31|\n",
      "|          4|            0|          0|Beautiful watch f...| 2015-08-31|\n",
      "+-----------+-------------+-----------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = table.select([\"star_rating\", \"helpful_votes\", \"total_votes\", \"review_body\", \"review_date\"])\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwIhrrWlUevj",
    "outputId": "fcbd1f82-526e-4d07-9cba-7d09030c2b1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print our schema same idea as \"dtypes\" in pandas\n",
    "# Shows us the data types of the columns *AND* they can be changed.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EuRHATrKFQg"
   },
   "source": [
    "Use Pyspark to build Pipeline in order to Train Test written reviews with Bayes Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "KA7LAnBHKeTJ"
   },
   "outputs": [],
   "source": [
    "# Import the required dependencies\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF-sCG0WdchZ"
   },
   "source": [
    "Create a positive review class and a negative review class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaO6qwLHUtYu",
    "outputId": "d747ef8a-5bed-4c4f-d129-8dc84c24e62a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+--------------------+-----------+--------+\n",
      "|star_rating|helpful_votes|total_votes|         review_body|review_date|   class|\n",
      "+-----------+-------------+-----------+--------------------+-----------+--------+\n",
      "|          5|            0|          0|Absolutely love t...| 2015-08-31|positive|\n",
      "|          5|            0|          0|I love this watch...| 2015-08-31|positive|\n",
      "|          2|            1|          1|           Scratches| 2015-08-31|negative|\n",
      "|          5|            0|          0|It works well on ...| 2015-08-31|positive|\n",
      "|          4|            0|          0|Beautiful watch f...| 2015-08-31|positive|\n",
      "|          5|            0|          0|i love this watch...| 2015-08-31|positive|\n",
      "|          5|            1|          1|for my wife and s...| 2015-08-31|positive|\n",
      "|          1|            1|          5|I was about to bu...| 2015-08-31|negative|\n",
      "|          5|            1|          2|Watch is perfect....| 2015-08-31|positive|\n",
      "|          4|            0|          0|Great quality and...| 2015-08-31|positive|\n",
      "|          4|            2|          2|The watch was pre...| 2015-08-31|positive|\n",
      "|          1|            0|          0|I bought this wat...| 2015-08-31|negative|\n",
      "|          5|            0|          0|Heavier than i th...| 2015-08-31|positive|\n",
      "|          5|            2|          3|Had it for severa...| 2015-08-31|positive|\n",
      "|          5|            0|          0|This one is diffe...| 2015-08-31|positive|\n",
      "|          4|            1|          1|The watch is attr...| 2015-08-31|positive|\n",
      "|          5|            0|          0|      said my wife..| 2015-08-31|positive|\n",
      "|          5|            0|          0|Nice watch, on ti...| 2015-08-31|positive|\n",
      "|          4|            0|          0|Looks great and l...| 2015-08-31|positive|\n",
      "|          4|            1|          1|I really like thi...| 2015-08-31|positive|\n",
      "+-----------+-------------+-----------+--------------------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.where(df.star_rating != 3)\n",
    "df = df.withColumn('class', f.when(f.col('star_rating') > 3, \"positive\").otherwise(\"negative\"))\n",
    "\n",
    "#df = df.withColumn((\"class\", \"positive\").where(df.star_rating > 3) and (\"class\", \"negative\").where(df.star_rating < 3))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0aXAXF_K9Ks",
    "outputId": "267a483c-aed2-41a1-8d2a-500e7982b333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+------+\n",
      "|star_rating|helpful_votes|total_votes|review_body                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |review_date|class   |length|\n",
      "+-----------+-------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+------+\n",
      "|5          |0            |0          |Absolutely love this watch! Get compliments almost every time I wear it. Dainty.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |2015-08-31 |positive|80    |\n",
      "|5          |0            |0          |I love this watch it keeps time wonderfully.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |2015-08-31 |positive|44    |\n",
      "|2          |1            |1          |Scratches                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |2015-08-31 |negative|9     |\n",
      "|5          |0            |0          |It works well on me. However, I found cheaper prices in other places after making the purchase                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |2015-08-31 |positive|94    |\n",
      "|4          |0            |0          |Beautiful watch face.  The band looks nice all around.  The links do make that squeaky cheapo noise when you swing it back and forth on your wrist which can be embarrassing in front of watch enthusiasts.  However, to the naked eye from afar, you can't tell the links are cheap or folded because it is well polished and brushed and the folds are pretty tight for the most part.<br /><br />I love the new member of my collection and it looks great.  I've had it for about a week and so far it has kept good time despite day 1 which is typical of a new mechanical watch|2015-08-31 |positive|566   |\n",
      "+-----------+-------------+-----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a column for length of body text\n",
    "df = df.withColumn(\"length\", length(df[\"review_body\"]))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5atk7CE0gHSU",
    "outputId": "74841b3f-addc-44a4-89c2-da889340f647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------+\n",
      "|   class|         review_body|length|\n",
      "+--------+--------------------+------+\n",
      "|positive|Absolutely love t...|    80|\n",
      "|positive|I love this watch...|    44|\n",
      "|negative|           Scratches|     9|\n",
      "|positive|It works well on ...|    94|\n",
      "|positive|Beautiful watch f...|   566|\n",
      "|positive|i love this watch...|   127|\n",
      "|positive|for my wife and s...|    60|\n",
      "|negative|I was about to bu...|   244|\n",
      "|positive|Watch is perfect....|   322|\n",
      "|positive|Great quality and...|   145|\n",
      "|positive|The watch was pre...|   550|\n",
      "|negative|I bought this wat...|   446|\n",
      "|positive|Heavier than i th...|    21|\n",
      "|positive|Had it for severa...|   144|\n",
      "|positive|This one is diffe...|   247|\n",
      "|positive|The watch is attr...|   134|\n",
      "|positive|      said my wife..|    14|\n",
      "|positive|Nice watch, on ti...|    41|\n",
      "|positive|Looks great and l...|   225|\n",
      "|positive|I really like thi...|   269|\n",
      "+--------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select([\"class\", \"review_body\", \"length\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "vNQmNdp1JTk8"
   },
   "outputs": [],
   "source": [
    "# Import pipeline dependencies\n",
    "from pyspark.ml.feature import StringIndexer as SI\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover as stop\n",
    "from pyspark.ml.feature import HashingTF as hash\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "IuzewHzRkX0L"
   },
   "outputs": [],
   "source": [
    "# Create all the features to the data set to pipeline\n",
    "index = SI(inputCol=\"class\", outputCol=\"target\")\n",
    "tokenizer = Tokenizer(inputCol=\"review_body\", outputCol=\"token_text\")\n",
    "stopper = stop(inputCol=\"token_text\", outputCol=\"stop_tokens\")\n",
    "hasher = hash(inputCol=\"stop_tokens\", outputCol=\"hash_token\")\n",
    "idf = IDF(inputCol=\"hash_token\", outputCol=\"idf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "9BvWyg4NiG8s"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler as VA\n",
    "# from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "assemble = VA(inputCols=[\"idf_token\", \"length\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "OxQuxn3ZJToy"
   },
   "outputs": [],
   "source": [
    "# Create and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline_steps = Pipeline(stages=[index, tokenizer, stopper, hasher, idf, assemble])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QA0Au-PgJTre",
    "outputId": "62405623-9eee-4180-b9ba-bd3940b72b5b"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-8c4f391dbb86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit and transform the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcleaner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.2.2-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/content/spark-3.2.2-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.2.2-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/content/spark-3.2.2-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.2.2-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.2.2-bin-hadoop2.7/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.2.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.2.2-bin-hadoop2.7/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1334.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 76.0 failed 1 times, most recent failure: Lost task 0.0 in stage 76.0 (TID 67) (6bed59ba5b4b executor driver): org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$3394/826047598: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 30 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:55)\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:93)\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:69)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$3394/826047598: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner = pipeline_steps.fit(df)\n",
    "cleaned = cleaner.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XhBYNRkJTwH"
   },
   "source": [
    "I do not know why this cell is not working. With the same format, it did work in a previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1X2fV0tJTuD"
   },
   "outputs": [],
   "source": [
    "# Show label and pipeline result features\n",
    "cleaned_df = cleaned.select([\"target\", \"features\"])\n",
    "cleaned_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYodPrRGDIHZ"
   },
   "outputs": [],
   "source": [
    "# Break into training and testing sets\n",
    "\n",
    "# Import dependency \n",
    "from pyspark.ml.classification import NaiveBayes as nb\n",
    "\n",
    "# Break data data into training set and test set\n",
    "train, test = cleaned.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create a Naive Bayes model and fit the training data\n",
    "predictor = nb.fit(train)\n",
    "\n",
    "# Transform the model with the test data\n",
    "test_results = predictor.transform(test)\n",
    "test_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyeGBwrON039"
   },
   "outputs": [],
   "source": [
    "# Use pyspark to Evalute Results\n",
    "\n",
    "# Import Evaluator tool dependency\n",
    "from pyspark.ml.evaluation import MultiClassificationEvaluator as mce\n",
    "\n",
    "accuracy=mce.evalutate(test_results)\n",
    "print(\"Accuracy of model at predicting reviews was :%f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ey1I5GhFO4oq"
   },
   "source": [
    "Results would be super awesome to analyze here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "336Wq3arJTzB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmQQVYmCJT2_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vq-kfT6yUewG"
   },
   "source": [
    "### Accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leLzlTgfUewL"
   },
   "outputs": [],
   "source": [
    "dataframe['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ut-wveumUewQ"
   },
   "outputs": [],
   "source": [
    "type(dataframe['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZi9NQGwUewU"
   },
   "outputs": [],
   "source": [
    "dataframe.select('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyUeV2G5UewX"
   },
   "outputs": [],
   "source": [
    "type(dataframe.select('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMd-MI_yUewj"
   },
   "outputs": [],
   "source": [
    "dataframe.select('price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO2MMkR1Uewm"
   },
   "source": [
    "### Manipulating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UuzYlpNwUewo"
   },
   "outputs": [],
   "source": [
    "# Add new column\n",
    "dataframe.withColumn('newprice', dataframe['price']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nMI3hXvUewt"
   },
   "outputs": [],
   "source": [
    "# Update column name\n",
    "dataframe.withColumnRenamed('price','newerprice').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4wkcAo7Uewz"
   },
   "outputs": [],
   "source": [
    "# Double the price\n",
    "dataframe.withColumn('doubleprice',dataframe['price']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCKNp0aEUew3"
   },
   "outputs": [],
   "source": [
    "# Add a dollar to the price\n",
    "dataframe.withColumn('add_one_dollar',dataframe['price']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uppVIRZ3Uew8"
   },
   "outputs": [],
   "source": [
    "# Half the price\n",
    "half_df = dataframe.withColumn('half_price',dataframe['price']/2)\n",
    "half_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_p_Fft--Uew_"
   },
   "outputs": [],
   "source": [
    "# Collecting a column as a list\n",
    "dataframe.select(\"price\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJMlQ0Njrpfi"
   },
   "outputs": [],
   "source": [
    "half_df.select(avg(\"half_price\"), avg(\"price\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00Do9WvMvxJ5"
   },
   "outputs": [],
   "source": [
    "# Tokenize / split to individual words\n",
    "\n",
    "# import first\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"<-column with sentence here->\", outputCol=\"<-new token *words* column here->\") # Change\n",
    "# transform and show data\n",
    "tokenized = tokenizer.transform(dataframe)\n",
    "tokenized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I033a6Jv745"
   },
   "outputs": [],
   "source": [
    "# Import UDF tool\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "# Create a function\n",
    "def list_length(word_list):\n",
    "  return len(word_list)  ## Note length is a built in function so we might not use this \"withColumn\" tool\n",
    "\n",
    "# Create UDF (user defined function)\n",
    "token_count = udf(list_length, IntegerType())\n",
    "count_tokenized = tokenized.select(\"<-column with sentence here->\", \"<-new *words*-here\").withColumn(\"<-token count here->\", token_count(col(\"<new*words\")))\n",
    "count_tokenized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W46jyAsRz5BQ"
   },
   "outputs": [],
   "source": [
    "# Import StopWords ***** Probably want to do this before Tokenizing *****\n",
    "from pyspark.ml.feature import StopWordsRemover as stop\n",
    "\n",
    "# Instantiate the remover\n",
    "remove = stop(inputCol=\"column with sentence here->\", outputCol=\"<-filtered old column->\")\n",
    "filtered_df = remove.transform(dataframe)\n",
    "filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UCN5VoQ3GzU"
   },
   "outputs": [],
   "source": [
    "# TF IDF Term Frequency / Inverse Document Frequency\n",
    "# 2 to the 4th can be adjusted upward as needed\n",
    "# OR allow it to default by not specifying ( 262145 buckets )\n",
    "\n",
    "# Import Hashing Term Frequency tool\n",
    "from pyspark.ml.feature import HashingTF as hash\n",
    "hashing = hash(inpupCol=\"<-filtered old column here->\", outputCol=\"<- new*values*here->\", numFeatures=pow(2,4)) # 2 to the 4th\n",
    "\n",
    "# Transform new column into the dataframe and show\n",
    "hashed_df = hashing.transform(filtered_df)\n",
    "hashed_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44HxCOio7hJm"
   },
   "outputs": [],
   "source": [
    "# Fit transform to the data\n",
    "idf = IDF(inputCol=\"<-new*values*here->\", \"features\")\n",
    "idfModel = idf.fit(hashed_df)\n",
    "rescaleData = idfModel.transform(hashed_df)\n",
    "\n",
    "# Display the scaled data\n",
    "scaled_df = rescaleData.select(\"<-filtered*words*column-here->\", \"features\")\n",
    "scaled_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWxnRO37IC2V"
   },
   "outputs": [],
   "source": [
    "# Sample Reference df.show()\n",
    "\n",
    "## This class example df name had\n",
    "# col 1 a pos/neg label column called \"class**\"\n",
    "# col 2 a sentence column called \"text**\"\n",
    "# col 3 a length column called \"length**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZeyZ9QqDH6d"
   },
   "outputs": [],
   "source": [
    "# Use pyspark to ..somethinghere... Bayes Theorem\n",
    "\n",
    "# Import String Indexer tool\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector # this is actaully not needed here\n",
    "from pyspark.ssql.functions import length ## Or use the function code from above?\n",
    "\n",
    "# Create all the features\n",
    "pos_neg_to_num = StringIndexer(inputCol=\"<-class**_orwhatever->\", outputCol=\"label\") # could also call label \"target?\"\n",
    "\n",
    "## Mabye some of this also from above\n",
    "tokenizer = Tokenizer(inputCol=\"text**_orwhatever\", outputCol=\"token_text_orwhatever\")\n",
    "stopremove = stop(inputCol=\"token_text_orwhatever\", outputCol=\"stop_tokens_orwhatever\")\n",
    "hashingTF = hash(inputCol=\"stop_tokens_orwhatever\", outputCol=\"hash_token_orwhatever\")\n",
    "idf = IDF(inputCol=\"hash_token_orwhatever\", outputCol=\"idf_token_orwhatever\")\n",
    "\n",
    "# Clean Up\n",
    "clean_up = VectorAssembler(inputCol=[\"idf_token_orwhatever\", \"length**_orwhatever\"], outputCol=\"features\")\n",
    "\n",
    "# Create and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, cleanup])\n",
    "\n",
    "# Fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(df)\n",
    "cleaned = cleaner.transform(df)\n",
    "\n",
    "# Show label and pipeline result features\n",
    "cleaned_df = cleaned.select([\"label\", \"features\"])\n",
    "cleaned_df.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
    "theme": {
     "0535adbc-b74f-46cc-9cd6-4eabe2477c8e": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         43,
         43,
         43
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         238,
         238
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         19,
         218,
         236
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         238,
         238,
         238
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     },
     "cc59980f-cb69-400a-b63a-1fb85ca73c8a": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "cc59980f-cb69-400a-b63a-1fb85ca73c8a",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  },
  "nteract": {
   "version": "0.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
